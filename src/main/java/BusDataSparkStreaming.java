import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;

import java.util.concurrent.TimeoutException;

import static org.apache.spark.sql.functions.*;

public class BusDataSparkStreaming {
    public static void main(String[] args) throws StreamingQueryException {

        SparkSession spark = SparkSession.builder()
                .appName("BusDataStreaming")
                .master("local[*]")
                .getOrCreate();

        spark.sparkContext().setLogLevel("WARN");

        System.out.println("‚è≥ Spark –∑–∞–ø—É—â–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ TCP-—Å–µ—Ä–≤–µ—Ä—É...");

        // –°—Ö–µ–º–∞ –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö JSON –¥–∞–Ω–Ω—ã—Ö
        StructType schema = new StructType()
                .add("internalRouteId", DataTypes.StringType)
                .add("realRouteNumber", DataTypes.StringType)
                .add("x", DataTypes.DoubleType)
                .add("y", DataTypes.DoubleType)
                .add("s", DataTypes.IntegerType)
                .add("r", DataTypes.BooleanType)
                .add("h", DataTypes.IntegerType)
                .add("u", DataTypes.StringType)
                .add("ts", DataTypes.LongType)
                .add("b", DataTypes.LongType)
                .add("d", DataTypes.IntegerType)
                .add("g", DataTypes.StringType)
                .add("px", DataTypes.DoubleType)
                .add("py", DataTypes.DoubleType)
                .add("l", DataTypes.StringType);

        // –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–æ–∫–µ—Ç—É (TCP-—Å–µ—Ä–≤–µ—Ä—É)
        Dataset<String> lines = spark.readStream()
                .format("socket")
                .option("host", "localhost")
                .option("port", 9999)
                .load()
                .as(Encoders.STRING());

        // –ü–∞—Ä—Å–∏–º JSON –≤ DataFrame
        Dataset<Row> busData = lines.select(from_json(col("value"), schema).alias("data"))
                .select("data.*")
                .withColumn("eventTime",
                        to_timestamp(from_unixtime(col("ts"), "yyyy-MM-dd HH:mm:ss"))
                                .cast(DataTypes.TimestampType));

        // –ó–∞–ø—É—Å–∫ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        StreamingQuery query = null;
        try {
            query = busData.writeStream()
                    .foreachBatch((dataset, batchId) -> {
                        long count = dataset.count();
                        System.out.printf("üîÑ Batch #%d —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω. –ó–∞–ø–∏—Å–µ–π –≤ –±–∞—Ç—á–µ: %d%n", batchId, count);
                        if (count > 0) {
                            dataset.show(false);
                            dataset.write()
                                    .mode("append")
                                    .parquet("bus-data-parquet");
                            System.out.printf("‚úÖ –î–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞ #%d —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ parquet%n", batchId);
                        } else {
                            System.out.printf("‚ö†Ô∏è –í –±–∞—Ç—á–µ #%d –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç%n", batchId);
                        }
                    })
                    .option("checkpointLocation", "bus-data-checkpoint")
                    .start();
        } catch (TimeoutException e) {
            throw new RuntimeException(e);
        }

        System.out.println("üöÄ Spark Streaming —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω –∏ —Å–ª—É—à–∞–µ—Ç –ø–æ—Ä—Ç 9999");

        query.awaitTermination();
    }
}
